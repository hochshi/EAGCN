{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib; matplotlib.use('agg')\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from models import *\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from rdkit import DataStructs\n",
    "\n",
    "random_state = 11\n",
    "nBits = [64, 128, 256, 512, 1024]\n",
    "batch_size = 100\n",
    "num_epochs = 80\n",
    "weight_decay = 0.0001  # L-2 Norm\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels):\n",
    "    X, x_test, y, y_test = train_test_split(data, labels, test_size=0.1, random_state=random_state, stratify=labels)\n",
    "    \n",
    "#     tensor_x_test = torch.from_numpy(x_test).float()\n",
    "#     tensor_y_test = torch.from_numpy(y_test).long()\n",
    "#     test_dataset = torch.utils.data.TensorDataset(tensor_x_test, tensor_y_test)\n",
    "#     test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=random_state, stratify=y)\n",
    "    \n",
    "#     tensor_x_val = torch.from_numpy(x_val).float()\n",
    "#     tensor_y_val = torch.from_numpy(y_val).long()\n",
    "#     val_dataset = torch.utils.data.TensorDataset(tensor_x_val, tensor_y_val)\n",
    "#     val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size)\n",
    "    \n",
    "#     tensor_x_train = torch.from_numpy(x_train).float()\n",
    "#     tensor_y_train = torch.from_numpy(y_train).long()\n",
    "#     train_dataset = torch.utils.data.TensorDataset(tensor_x_train, tensor_y_train)\n",
    "#     train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "    \n",
    "#     return (train_loader, val_loader, test_loader)\n",
    "    return ((x_train, y_train), (x_val, y_val), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dist_mat, train_size, train_labels, true_labels):\n",
    "# We assume that the dist matrix is 4 sub matrices, train vs. train train vs. something else.\n",
    "# So the first train_size X train_size entries are the similarity of the train vs. itself.\n",
    "    \n",
    "    sub_mat = dist_mat[train_size:, :train_size]\n",
    "    correct = [0]*4\n",
    "    total = dist_mat.shape[0] - train_size\n",
    "    top_ks = [1, 5, 10, 30]\n",
    "#     top_ks = np.array(top_ks) - 1\n",
    "    for i, topk in enumerate(top_ks):\n",
    "        nn = np.partition(sub_mat, topk, axis=1)\n",
    "        nn = sub_mat < nn[:,topk].reshape(-1, 1)\n",
    "        nn_labels = np.matmul(nn, train_labels)\n",
    "        correct[i] = (np.multiply(nn_labels, true_labels) > 0).sum()\n",
    "    return np.true_divide(correct,total).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading small_batch_test dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "x_all, y_all, target, sizes, mol_to_graph_transform, parameter_holder, edge_vocab, node_vocab = \\\n",
    "    load_data('small_batch_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "[0.6714120815698346, 0.8410927279722971, 0.8518661023470565, 0.8726433243555214]\n",
      "[0.6692067890543817, 0.8330446830620021, 0.8514028403186699, 0.8708001385521302]\n",
      "128\n",
      "[0.7325894574836476, 0.868410927279723, 0.8811081185071181, 0.8976529434397845]\n",
      "[0.7235885001731902, 0.8638725320401801, 0.8791132663664704, 0.8957395219951507]\n",
      "256\n",
      "[0.7387456714120816, 0.8745671412081569, 0.8895729126587149, 0.9118891881492882]\n",
      "[0.7346726705923103, 0.8714928992033253, 0.885001731901628, 0.9037062694838933]\n",
      "1024\n",
      "[0.7391304347826086, 0.8803385917660639, 0.893035782993459, 0.9157368218545594]\n",
      "[0.7357118115691029, 0.8756494631104953, 0.8888119154832006, 0.9078628333910634]\n"
     ]
    }
   ],
   "source": [
    "for nbits in nBits:\n",
    "    \n",
    "    fps = [AllChem.GetMorganFingerprintAsBitVect(MolFromInchi(mol_dat[-1]), 2, nBits=nbits) for mol_dat in x_all]\n",
    "    len_train = len(fps)\n",
    "    labels = y_all\n",
    "\n",
    "    np_fps = []\n",
    "    for fp in fps:\n",
    "      arr = np.zeros((1,), dtype=np.int8)\n",
    "      DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "      np_fps.append(arr)\n",
    "\n",
    "    np_fps = np.concatenate(np_fps, axis=0)\n",
    "    np_fps = np_fps.reshape(-1, nbits)\n",
    "\n",
    "    np_fps, labels = shuffle(np_fps, labels, random_state=random_state)\n",
    "    train_loader, validation_loader, test_loader = split_data(np_fps, labels)\n",
    "    train_val = np.concatenate((train_loader[0], validation_loader[0]))\n",
    "    train_val = squareform(pdist(train_val, 'jaccard'))\n",
    "\n",
    "    train_test = np.concatenate((train_loader[0], test_loader[0]))\n",
    "    train_test = squareform(pdist(train_test, 'jaccard'))\n",
    "    \n",
    "    print(nbits)\n",
    "    print(test_model(train_val, train_loader[0].shape[0], train_loader[1], validation_loader[1]))\n",
    "    print(test_model(train_test, train_loader[0].shape[0], train_loader[1], test_loader[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = np.concatenate((train_loader[0], validation_loader[0]))\n",
    "train_val = squareform(pdist(train_val, 'jaccard'))\n",
    "\n",
    "train_test = np.concatenate((train_loader[0], test_loader[0]))\n",
    "train_test = squareform(pdist(train_test, 'jaccard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(train_val, train_loader[0].shape[0], train_loader[1], validation_loader[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(train_test, train_loader[0].shape[0], train_loader[1], test_loader[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
